{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17a3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from Efficient_Wav_FastKAN import *\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler  # Used for standardized processing\n",
    "from sklearn.metrics import confusion_matrix # Calculate the sensitivity and specificity\n",
    "import openpyxl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9915a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/30 | Train Loss: 0.4833 | Train Acc: 76.30% | \n",
      "Epoch 11/30 | Train Loss: 0.1394 | Train Acc: 99.48% | \n",
      "Epoch 21/30 | Train Loss: 0.1241 | Train Acc: 100.00% | \n",
      "Epoch 30/30 | Train Loss: 0.1079 | Train Acc: 100.00% | \n",
      "\n",
      "Fold 2\n",
      "Epoch 1/30 | Train Loss: 0.3162 | Train Acc: 84.90% | \n",
      "Epoch 11/30 | Train Loss: 0.1344 | Train Acc: 99.22% | \n",
      "Epoch 21/30 | Train Loss: 0.1065 | Train Acc: 99.74% | \n",
      "Epoch 30/30 | Train Loss: 0.1072 | Train Acc: 99.74% | \n",
      "\n",
      "Fold 3\n",
      "Epoch 1/30 | Train Loss: 0.4139 | Train Acc: 81.51% | \n",
      "Epoch 11/30 | Train Loss: 0.1359 | Train Acc: 98.70% | \n",
      "Epoch 21/30 | Train Loss: 0.1197 | Train Acc: 99.74% | \n",
      "Epoch 30/30 | Train Loss: 0.1088 | Train Acc: 99.48% | \n",
      "\n",
      "Fold 4\n",
      "Epoch 1/30 | Train Loss: 0.2858 | Train Acc: 87.50% | \n",
      "Epoch 11/30 | Train Loss: 0.1292 | Train Acc: 99.48% | \n",
      "Epoch 21/30 | Train Loss: 0.1103 | Train Acc: 100.00% | \n",
      "Epoch 30/30 | Train Loss: 0.1003 | Train Acc: 100.00% | \n",
      "\n",
      "Fold 5\n",
      "Epoch 1/30 | Train Loss: 0.3521 | Train Acc: 83.07% | \n",
      "Epoch 11/30 | Train Loss: 0.1391 | Train Acc: 98.70% | \n",
      "Epoch 21/30 | Train Loss: 0.1191 | Train Acc: 99.48% | \n",
      "Epoch 30/30 | Train Loss: 0.1040 | Train Acc: 100.00% | \n",
      "\n",
      "==============================\n",
      " Chb: 1\n",
      "Accuracy of the 5-fold cross validation: ['100.00', '97.20', '100.00', '100.00', '100.00']\n",
      "Sensitivity of the 5-fold cross validation: ['100.00', '94.44', '100.00', '100.00', '100.00']\n",
      "Specificity of the 5-fold cross validation: ['100.00', '100.00', '100.00', '100.00', '100.00']\n",
      "Average_accuracy: 99.44 ± 1.121\n",
      "Average_sensitivity: 98.89 ± 2.222\n",
      "Average_specificity: 100.00 ± 0.000\n",
      "Average_inference_time：65.33ms\n",
      "\n",
      "Fold 1\n",
      "Epoch 1/30 | Train Loss: 0.1953 | Train Acc: 96.31% | \n",
      "Epoch 11/30 | Train Loss: 0.0724 | Train Acc: 100.00% | \n",
      "Epoch 21/30 | Train Loss: 0.0527 | Train Acc: 100.00% | \n",
      "Epoch 30/30 | Train Loss: 0.0424 | Train Acc: 100.00% | \n",
      "\n",
      "Fold 2\n",
      "Epoch 1/30 | Train Loss: 0.1957 | Train Acc: 95.99% | \n",
      "Epoch 11/30 | Train Loss: 0.0749 | Train Acc: 100.00% | \n",
      "Epoch 21/30 | Train Loss: 0.0527 | Train Acc: 100.00% | \n",
      "Epoch 30/30 | Train Loss: 0.0445 | Train Acc: 100.00% | \n",
      "\n",
      "Fold 3\n",
      "Epoch 1/30 | Train Loss: 0.1920 | Train Acc: 96.02% | \n",
      "Epoch 11/30 | Train Loss: 0.0748 | Train Acc: 100.00% | \n",
      "Epoch 21/30 | Train Loss: 0.0545 | Train Acc: 100.00% | \n",
      "Epoch 30/30 | Train Loss: 0.0433 | Train Acc: 100.00% | \n",
      "\n",
      "Fold 4\n",
      "Epoch 1/30 | Train Loss: 0.2065 | Train Acc: 95.70% | \n",
      "Epoch 11/30 | Train Loss: 0.0740 | Train Acc: 100.00% | \n",
      "Epoch 21/30 | Train Loss: 0.0527 | Train Acc: 100.00% | \n",
      "Epoch 30/30 | Train Loss: 0.0439 | Train Acc: 100.00% | \n",
      "\n",
      "Fold 5\n",
      "Epoch 1/30 | Train Loss: 0.1797 | Train Acc: 97.16% | \n",
      "Epoch 11/30 | Train Loss: 0.0733 | Train Acc: 100.00% | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 219\u001b[0m\n\u001b[0;32m    216\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 219\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 199\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(root_dir, epochs, batch_size)\u001b[0m\n\u001b[0;32m    197\u001b[0m pd \u001b[38;5;241m=\u001b[39m root_dir\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pat_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m--> 199\u001b[0m     result, result_sen, result_spc, result_infer_time \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_patient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpat_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[9], line 164\u001b[0m, in \u001b[0;36mprocess_patient\u001b[1;34m(patient_dir, pat_id, epochs, batch_size)\u001b[0m\n\u001b[0;32m    161\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mExponentialLR(optimizer, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.96\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Training model\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m best_acc, best_sen, best_spc, inference_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m fold_acc\u001b[38;5;241m.\u001b[39mappend(best_acc)\n\u001b[0;32m    168\u001b[0m fold_sen\u001b[38;5;241m.\u001b[39mappend(best_sen)\n",
      "Cell \u001b[1;32mIn[9], line 54\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     53\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 54\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     56\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mD:\\ANACONDA\\envs\\pykan-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA\\envs\\pykan-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Jupyter_notebook\\Wav-KAN-main\\Efficient_Wav_FastKAN.py:501\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    500\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\ANACONDA\\envs\\pykan-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA\\envs\\pykan-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Jupyter_notebook\\Wav-KAN-main\\Efficient_Wav_FastKAN.py:436\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    434\u001b[0m def forward(self, x):\n\u001b[0;32m    435\u001b[0m     wavelet_output = self.wavelet_transform(x)\n\u001b[1;32m--> 436\u001b[0m     #You may like test the cases like Spl-KAN\n\u001b[0;32m    437\u001b[0m     #wav_output = F.linear(wavelet_output, self.weight)\n\u001b[0;32m    438\u001b[0m     #base_output = F.linear(self.base_activation(x), self.weight1)\n\u001b[0;32m    440\u001b[0m     base_output = F.linear(x, self.weight1)\n\u001b[0;32m    441\u001b[0m     combined_output =  wavelet_output #+ base_output \n",
      "File \u001b[1;32mD:\\Jupyter_notebook\\Wav-KAN-main\\Efficient_Wav_FastKAN.py:399\u001b[0m, in \u001b[0;36mwavelet_transform\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwavelet_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Implementing Derivative of Gaussian Wavelet \u001b[39;00m\n\u001b[0;32m    398\u001b[0m     dog \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mx_scaled \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m x_scaled \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)         \n\u001b[1;32m--> 399\u001b[0m     wavelet \u001b[38;5;241m=\u001b[39m dog\n\u001b[0;32m    400\u001b[0m     wavelet_weighted \u001b[38;5;241m=\u001b[39m wavelet \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwavelet_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(wavelet)\n\u001b[0;32m    401\u001b[0m     wavelet_output \u001b[38;5;241m=\u001b[39m wavelet_weighted\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mD:\\ANACONDA\\envs\\pykan-env\\lib\\site-packages\\torch\\_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## \n",
    "from contrast_mlpLayers import *\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set random seeds to ensure repeatability\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "class PatientDataset(Dataset):\n",
    "    \"\"\"Customize the PyTorch dataset\"\"\"\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample, label\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=30):\n",
    "    \"\"\"Train and validate the model\"\"\"\n",
    "    all_val_acc = []\n",
    "    all_val_sen = []\n",
    "    all_val_spc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    " \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_acc = 100 * train_correct / train_total \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()        \n",
    " \n",
    "        if epoch % 10 == 0 or epoch == epochs-1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.2f}% | \"\n",
    "\n",
    "                 )\n",
    "\n",
    "    # Validation\n",
    "    \n",
    "    # Save the relevant parameters of the trained model\n",
    "    torch.save(model,'KAN_variant.pth')\n",
    "    # Model instantiation\n",
    "    model_test = torch.load('KAN_variant.pth')\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_predicted = []\n",
    "    all_labels = []\n",
    "    inference_time = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            end = time.time()\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            all_predicted.extend(predicted.cpu().numpy().ravel().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().ravel().tolist())\n",
    "            \n",
    "            inference_time.append((end-start)*1000)\n",
    "    \n",
    "    # Calculate the average criteria   \n",
    "    val_loss = val_loss / len(val_loader.dataset)   \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    cm = confusion_matrix(all_labels, all_predicted)\n",
    "    tn, fp,  fn, tp = cm.ravel()\n",
    "    val_sen = tp / (tp + fn)\n",
    "    val_spc = tn / (tn + fp)\n",
    "    val_pr = tp / (tp + fp)\n",
    "    val_F1 = 2 * val_pr * val_sen / (val_pr + val_sen)\n",
    "    \n",
    "    return val_acc, val_sen*100, val_spc*100, np.mean(inference_time)\n",
    " \n",
    "def process_patient(patient_dir, pat_id, epochs=30, batch_size=64):\n",
    "    \"\"\"Process individual patient data\"\"\"\n",
    "    try:\n",
    "        # Load data\n",
    "        combined = loadmat(os.path.join(patient_dir, f'data_combined_shuffled_{pat_id}.mat'))['data_combined_shuffled'].astype(np.float32)\n",
    "        X = combined[:, :-1].astype(np.float32)\n",
    "        y = combined[:, -1].astype(np.int64)\n",
    " \n",
    "        # 5-fold cross validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        fold_acc = []\n",
    "        fold_sen = []\n",
    "        fold_spc = []\n",
    "        fold_infer_time = []\n",
    " \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            print(f\"\\nFold {fold+1}\")\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Standardization\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "           \n",
    " \n",
    "            # Create a data set\n",
    "            train_dataset = PatientDataset(X_train_scaled, y_train)\n",
    "            val_dataset = PatientDataset(X_val_scaled, y_val)\n",
    " \n",
    "            # Create loader\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    " \n",
    "            # Initialize the model\n",
    "            # Wav-KAN(Wav) or FastKAN(FastKAN) or Efficient-KAN(Spline)\n",
    "            input_dim = X_train_scaled.shape[1]\n",
    "            model = KAN([input_dim, 512, 2], wavelet_type='dog', kan_type='Wav')             \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.96)\n",
    "            \n",
    "            # Training model\n",
    "            best_acc, best_sen, best_spc, inference_time = train_model(model, train_loader, val_loader, \n",
    "                                 criterion, optimizer, scheduler, epochs)\n",
    "            \n",
    "            fold_acc.append(best_acc)\n",
    "            fold_sen.append(best_sen)\n",
    "            fold_spc.append(best_spc)\n",
    "            fold_infer_time.append(inference_time)\n",
    " \n",
    "        # Output the patient results\n",
    "        print(f\"\\n{'='*30}\\n Chb: {pat_id}\")\n",
    "        print(f\"Accuracy of the 5-fold cross validation: {[f'{acc:.2f}' for acc in fold_acc]}\")\n",
    "        print(f\"Sensitivity of the 5-fold cross validation: {[f'{sen:.2f}' for sen in fold_sen]}\")\n",
    "        print(f\"Specificity of the 5-fold cross validation: {[f'{spc:.2f}' for spc in fold_spc]}\")\n",
    "        \n",
    "        print(f\"Average_accuracy: {np.mean(fold_acc):.2f} ± {np.std(fold_acc):.3f}\")\n",
    "        print(f\"Average_sensitivity: {np.mean(fold_sen):.2f} ± {np.std(fold_sen):.3f}\")\n",
    "        print(f\"Average_specificity: {np.mean(fold_spc):.2f} ± {np.std(fold_spc):.3f}\")\n",
    "        print(f\"Average_inference_time：{np.mean(fold_infer_time):.2f}ms\")\n",
    "        \n",
    "        return np.mean(fold_acc), np.mean(fold_sen), np.mean(fold_spc), np.mean(fold_infer_time)\n",
    " \n",
    "    except Exception as e:\n",
    "        print(f\"\\nHanding {patient_dir} has a error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main(root_dir, epochs=30, batch_size=64):\n",
    "    \"\"\"Main processing function\"\"\"\n",
    " \n",
    "    all_results = []\n",
    "    all_results_sen = []\n",
    "    all_results_spc = []\n",
    "    all_results_infer_time = []\n",
    "    \n",
    "    pd = root_dir\n",
    "    for pat_id in range(1,30):\n",
    "        result, result_sen, result_spc, result_infer_time = process_patient(pd, pat_id, epochs, batch_size)\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "            all_results_sen.append(result_sen)\n",
    "            all_results_spc.append(result_spc)\n",
    "            all_results_infer_time.append(result_infer_time)\n",
    " \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Overall average accuracy: {np.mean(all_results):.4f} ± {np.std(all_results):.4f}\")\n",
    "    print(f\"Overall average sensitivity: {np.mean(all_results_sen):.4f} ± {np.std(all_results_sen):.4f}\")\n",
    "    print(f\"Overall average specificity: {np.mean(all_results_spc):.4f} ± {np.std(all_results_spc):.4f}\")\n",
    "    print(f\"Overall average inference time: {np.mean(all_results_infer_time):.4f}ms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration parameters\n",
    "    ROOT_DIR = \"E:/BaiduSyncdisk/EEG/Huashan_data\"\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 64\n",
    " \n",
    "    print(f\"Device: {device}\")\n",
    "    main(ROOT_DIR, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1f7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 0.56M\n",
      "Trainable Parameters: 0.56M\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of model parameters (Parameters)\n",
    "from thop import clever_format, profile\n",
    "\n",
    "model_complexity =torch.load('KAN_variant.pth')\n",
    "total_params = sum(p.numel() for p in model_complexity.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_complexity.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params/1e6:.2f}M\")\n",
    "print(f\"Trainable Parameters: {trainable_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28613200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykan (Python 3.9.7)",
   "language": "python",
   "name": "pykan-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
